{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb66d33e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "from datetime import datetime\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import os\n",
    "\n",
    "# Set variables for the whole process\n",
    "user = 'Insert Here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0047ba",
   "metadata": {},
   "source": [
    "# Delete Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete collection before inserting new data\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "client.drop_database('Stocks_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b737e",
   "metadata": {},
   "source": [
    "## Stocks Summary Table - ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed410fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CBA  ###############\n",
    "##### Extract #####\n",
    "# start web browser\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "stock = 'CBA'\n",
    "\n",
    "# get source code\n",
    "browser.get(f\"https://au.finance.yahoo.com/quote/{stock}.AX?p={stock}.AX\")\n",
    "html = browser.page_source\n",
    "time.sleep(2)\n",
    "df = pd.read_html(html)\n",
    "\n",
    "# close web browser\n",
    "browser.close()\n",
    "\n",
    "# Get each table from the table list\n",
    "summary_table_1 = df[0]\n",
    "summary_table_2 = df[1]\n",
    "# Join the tables together\n",
    "total_summary_table = summary_table_1.append(summary_table_2)\n",
    "# Rename the columns\n",
    "total_summary_table = total_summary_table.rename(columns = {0:'Summary Metric', 1:'Value'})\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "summary_table_new = total_summary_table[total_summary_table['Value']!=''].copy()\n",
    "\n",
    "# Replace all fields containing '-' with 0\n",
    "summary_table_new = summary_table_new.replace('-',0)\n",
    "\n",
    "# Set the index to Summary Metric\n",
    "summary_table_new = summary_table_new.set_index('Summary Metric')\n",
    "\n",
    "# Make it a series\n",
    "summary_table_stock_value = summary_table_new[['Value']].copy()\n",
    "\n",
    "# Convert it to a dictionary\n",
    "summary_df = summary_table_stock_value.to_dict()\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Drop existing Database\n",
    "client.Stocks_db.summary.drop()\n",
    "\n",
    "# Insert into database\n",
    "stock_summary_report={}\n",
    "stock_summary_report[stock] = summary_df\n",
    "\n",
    "client.Stocks_db.summary.insert_one(stock_summary_report)\n",
    "##### Load #####\n",
    "###############  CBA   ###############\n",
    "\n",
    "############################################################\n",
    "\n",
    "###############  ANZ   ###############\n",
    "##### Extract #####\n",
    "# start web browser\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "stock = 'ANZ'\n",
    "\n",
    "# get source code\n",
    "browser.get(f\"https://au.finance.yahoo.com/quote/{stock}.AX?p={stock}.AX\")\n",
    "html = browser.page_source\n",
    "time.sleep(2)\n",
    "df = pd.read_html(html)\n",
    "\n",
    "# close web browser\n",
    "browser.close()\n",
    "\n",
    "# Get each table from the table list\n",
    "summary_table_1 = df[0]\n",
    "summary_table_2 = df[1]\n",
    "# Join the tables together\n",
    "total_summary_table = summary_table_1.append(summary_table_2)\n",
    "# Rename the columns\n",
    "total_summary_table = total_summary_table.rename(columns = {0:'Summary Metric', 1:'Value'})\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "summary_table_new = total_summary_table[total_summary_table['Value']!=''].copy()\n",
    "\n",
    "# Replace all fields containing '-' with 0\n",
    "summary_table_new = summary_table_new.replace('-',0)\n",
    "\n",
    "# Set the index to Summary Metric\n",
    "summary_table_new = summary_table_new.set_index('Summary Metric')\n",
    "\n",
    "# Make it a series\n",
    "summary_table_stock_value = summary_table_new[['Value']].copy()\n",
    "\n",
    "# Convert it to a dictionary\n",
    "summary_df = summary_table_stock_value.to_dict()\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Insert into database\n",
    "stock_summary_report={}\n",
    "stock_summary_report[stock] = summary_df\n",
    "\n",
    "client.Stocks_db.summary.insert_one(stock_summary_report)\n",
    "##### Load #####\n",
    "###############  ANZ  ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b57be",
   "metadata": {},
   "source": [
    "## Stock History Year Month Average - ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CBA   ###############\n",
    "##### Extract #####\n",
    "# Get the current time in unixtime format\n",
    "d = datetime.today()\n",
    "unixtime = time.mktime(d.timetuple())\n",
    "starting_date = round(time.mktime(datetime.strptime(\"01/10/1991\", \"%d/%m/%Y\").timetuple()))\n",
    "current_time = round(unixtime)\n",
    "\n",
    "file_directory = 'Desktop\\\\yahoo-finance-anz-cba-etl'\n",
    "stock = 'CBA'\n",
    "\n",
    "# Change the download directory\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : f\"C:\\\\Users\\\\{user}\\\\{file_directory}\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=chromeOptions)\n",
    "\n",
    "# Remove the stock history CSV file if it already exists\n",
    "filename = f'{stock}.AX.csv'\n",
    "filepath = os.path.join(f\"C:\\\\Users\\\\{user}\\\\{file_directory}\", filename)\n",
    "if os.path.exists(filepath):\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Get the stock history CSV file\n",
    "url = f'https://query1.finance.yahoo.com/v7/finance/download/{stock}.AX?period1={starting_date}&period2={current_time}&interval=1d&events=history&includeAdjustedClose=true'\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Get the stock\n",
    "df = pd.read_csv(f'{stock}.AX.csv')\n",
    "# Get the dates and the year of each record\n",
    "df['Year']= df['Date'].str.slice(start=0,stop=4)\n",
    "df['Month']= df['Date'].str.slice(start=5,stop=7)\n",
    "# Remove all the null values\n",
    "df = df.dropna()\n",
    "# Get the average 'Open', 'High', 'Low', 'Close', 'Adj Close', and 'Volume' of each month for each year\n",
    "df_new = df.groupby(['Year','Month']).mean().round(6)\n",
    "# Round the average 'Volume' to 0 decimal places\n",
    "df_new['Volume'] = df_new['Volume'].round()\n",
    "# Round the average 'Open', 'High', 'Low', 'Close', and 'Adj Close' prices to 2 decimal places\n",
    "df_new[['Open', 'High', 'Low', 'Close', 'Adj Close']] = df_new[['Open', 'High', 'Low', 'Close', 'Adj Close']].round(2)\n",
    "# Create a new index called 'Year-Month-Average'\n",
    "df_new = df_new.reset_index()\n",
    "df_new['Year-Month-Average'] = df_new['Year'] + '-' + df_new['Month']\n",
    "df_new = df_new.drop(['Year','Month'],axis=1)\n",
    "df_new = df_new.set_index('Year-Month-Average')\n",
    "# Transpose the columns\n",
    "df_new_tran = df_new.copy().T\n",
    "#Convert to a dictionary\n",
    "df_dict = df_new_tran.to_dict()\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "# Load Into Mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "stock_dict[stock] = df_dict\n",
    "\n",
    "client.Stocks_db.stock_history_average.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  CBA   ###############\n",
    "\n",
    "###############  ANZ   ###############\n",
    "##### Extract #####\n",
    "# Get the current time in unixtime format\n",
    "d = datetime.today()\n",
    "unixtime = time.mktime(d.timetuple())\n",
    "starting_date = round(time.mktime(datetime.strptime(\"01/10/1991\", \"%d/%m/%Y\").timetuple()))\n",
    "current_time = round(unixtime)\n",
    "\n",
    "stock = 'ANZ'\n",
    "\n",
    "# Change the download directory\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : f\"C:\\\\Users\\\\{user}\\\\{file_directory}\"}\n",
    "chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=chromeOptions)\n",
    "\n",
    "# Remove the stock history CSV file if it already exists\n",
    "filename = f'{stock}.AX.csv'\n",
    "filepath = os.path.join(f\"C:\\\\Users\\\\{user}\\\\{file_directory}\", filename)\n",
    "if os.path.exists(filepath):\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Get the stock history CSV file\n",
    "url = f'https://query1.finance.yahoo.com/v7/finance/download/{stock}.AX?period1={starting_date}&period2={current_time}&interval=1d&events=history&includeAdjustedClose=true'\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Get the stock\n",
    "df = pd.read_csv(f'{stock}.AX.csv')\n",
    "# Get the dates and the year of each record\n",
    "df['Year']= df['Date'].str.slice(start=0,stop=4)\n",
    "df['Month']= df['Date'].str.slice(start=5,stop=7)\n",
    "# Remove all the null values\n",
    "df = df.dropna()\n",
    "# Get the average 'Open', 'High', 'Low', 'Close', 'Adj Close', and 'Volume' of each month for each year\n",
    "df_new = df.groupby(['Year','Month']).mean().round(6)\n",
    "# Round the average 'Volume' to 0 decimal places\n",
    "df_new['Volume'] = df_new['Volume'].round()\n",
    "# Round the average 'Open', 'High', 'Low', 'Close', and 'Adj Close' prices to 2 decimal places\n",
    "df_new[['Open', 'High', 'Low', 'Close', 'Adj Close']] = df_new[['Open', 'High', 'Low', 'Close', 'Adj Close']].round(2)\n",
    "# Create a new index called 'Year-Month-Average'\n",
    "df_new = df_new.reset_index()\n",
    "df_new['Year-Month-Average'] = df_new['Year'] + '-' + df_new['Month']\n",
    "df_new = df_new.drop(['Year','Month'],axis=1)\n",
    "df_new = df_new.set_index('Year-Month-Average')\n",
    "# Transpose the columns\n",
    "df_new_tran = df_new.copy().T\n",
    "#Convert to a dictionary\n",
    "df_dict = df_new_tran.to_dict()\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "# Load Into Mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "stock_dict[stock] = df_dict\n",
    "\n",
    "client.Stocks_db.stock_history_average.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  ANZ   ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2c47f",
   "metadata": {},
   "source": [
    "## Stocks Income statements - ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CBA  ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'CBA'\n",
    "\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/financials?p={stock}.AX'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "income_table_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "income_table_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in income_table_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "        \n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in income_table_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Convert the result into a DataFrame\n",
    "income_statements = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "#Find empty or NaN entry in Dataframe\n",
    "missing_cols, missing_rows = (\n",
    "    (income_statements.isnull().sum(x) | income_statements.eq('').sum(x))\n",
    "    .loc[lambda x: x.gt(0)].index\n",
    "    for x in (0, 1)\n",
    ")\n",
    "income_statements.loc[missing_rows, missing_cols]\n",
    "\n",
    "# Delete row with index label '1' and set new index \n",
    "Income_statements_df = income_statements.drop([1]).set_index('Breakdown')\n",
    "# Replace all fields containing '-' with 0\n",
    "Income_statements_df = Income_statements_df.replace('-',0)\n",
    "Income_statements_df\n",
    "\n",
    "# Create Series per year\n",
    "#2017-2018\n",
    "Income_statements_df_2018 = Income_statements_df[['29/06/2018']].copy()\n",
    "#2018-2019\n",
    "Income_statements_df_2019 = Income_statements_df[['29/06/2019']].copy()\n",
    "#2019-2020\n",
    "Income_statements_df_2020 = Income_statements_df[['29/06/2020']].copy()\n",
    "#2020-2021\n",
    "Income_statements_df_2021 = Income_statements_df[['29/06/2021']].copy()\n",
    "# TTM\n",
    "Income_statements_df_ttm = Income_statements_df[['ttm']].copy()\n",
    "\n",
    "# Convert them to dictionaries\n",
    "Income_statements_2018_dict = Income_statements_df_2018.to_dict()['29/06/2018']\n",
    "Income_statements_2019_dict = Income_statements_df_2019.to_dict()['29/06/2019']\n",
    "Income_statements_2020_dict = Income_statements_df_2020.to_dict()['29/06/2020']\n",
    "Income_statements_2021_dict = Income_statements_df_2021.to_dict()['29/06/2021']\n",
    "Income_statements_ttm_dict =  Income_statements_df_ttm.to_dict()['ttm']\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "#Convert series into dictionaries and group it into a one report/object\n",
    "stock_income_statements={'29/06/2018':Income_statements_2018_dict,\n",
    "                              '29/06/2019':Income_statements_2019_dict,\n",
    "                               '29/06/2020':Income_statements_2020_dict,\n",
    "                              '29/06/2021':Income_statements_2021_dict,\n",
    "                                'ttm':Income_statements_ttm_dict}\n",
    "stock_reports={}\n",
    "stock_reports[stock] = stock_income_statements\n",
    "\n",
    "#Insert object into MongoDB\n",
    "client.Stocks_db.income_statements.insert_one(stock_reports)\n",
    "##### Load #####\n",
    "###############  CBA   ###############\n",
    "\n",
    "############################################################\n",
    "\n",
    "###############  ANZ   ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'ANZ'\n",
    "\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/financials?p={stock}.AX'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "income_table_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "income_table_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in income_table_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "\n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in income_table_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Convert the result into a DataFrame\n",
    "income_statements = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "#Find empty or NaN entry in Dataframe\n",
    "missing_cols, missing_rows = (\n",
    "    (income_statements.isnull().sum(x) | income_statements.eq('').sum(x))\n",
    "    .loc[lambda x: x.gt(0)].index\n",
    "    for x in (0, 1)\n",
    ")\n",
    "income_statements.loc[missing_rows, missing_cols]\n",
    "\n",
    "# Delete row with index label '1' and set new index \n",
    "Income_statements_df = income_statements.drop([1]).set_index('Breakdown')\n",
    "\n",
    "# Replace all fields containing '-' with 0\n",
    "Income_statements_df = Income_statements_df.replace('-',0)\n",
    "\n",
    "#Create Dataframe/Series per year\n",
    "#2017-2018\n",
    "Income_statements_df_2018 = Income_statements_df[['29/09/2018']].copy()\n",
    "#2018-2019\n",
    "Income_statements_df_2019 = Income_statements_df[['29/09/2019']].copy()\n",
    "#2019-2020\n",
    "Income_statements_df_2020 = Income_statements_df[['29/09/2020']].copy()\n",
    "# TTM\n",
    "Income_statements_df_ttm = Income_statements_df[['ttm']].copy()\n",
    "\n",
    "# Convert series to dictionaries\n",
    "Income_statements_2018_dict = Income_statements_df_2018.to_dict()['29/09/2018']\n",
    "Income_statements_2019_dict = Income_statements_df_2019.to_dict()['29/09/2019']\n",
    "Income_statements_2020_dict = Income_statements_df_2020.to_dict()['29/09/2020']\n",
    "Income_statements_ttm_dict = Income_statements_df_ttm.to_dict()['ttm']\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "# The default port used by MongoDB is 27017\n",
    "# https://docs.mongodb.com/manual/reference/default-mongodb-port/\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "#Convert dataframe into dict and group it into a one report/object\n",
    "stock_income_statements = {'29/09/2018': Income_statements_2018_dict,\n",
    "                              '29/069/2019': Income_statements_2019_dict,\n",
    "                               '29/09/2020': Income_statements_2020_dict,\n",
    "                                'ttm': Income_statements_ttm_dict}\n",
    "stock_reports = {}\n",
    "stock_reports[stock] = stock_income_statements\n",
    "\n",
    "#Insert object into MongoDB\n",
    "client.Stocks_db.income_statements.insert_one(stock_reports)\n",
    "##### Load #####\n",
    "###############  ANZ   ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163f561",
   "metadata": {},
   "source": [
    "## Balance Sheets - ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b00adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CBA  ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'CBA'\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/balance-sheet?p={stock}.AX'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "balance_sheet_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "balance_sheet_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in balance_sheet_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "\n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in balance_sheet_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Convert the result into a DataFrame\n",
    "balance_table = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "balance_table_new = balance_table[balance_table['29/06/2021']!=''].copy()\n",
    "# Replace all fields containing '-' with 0\n",
    "balance_table_new = balance_table_new.replace('-',0)\n",
    "# Set the index to Breakdown\n",
    "balance_table_new = balance_table_new.set_index('Breakdown')\n",
    "\n",
    "# Divide into series by financial year\n",
    "#2017-2018\n",
    "balance_table_new_17to18 = balance_table_new[['29/06/2018']].copy()\n",
    "#2018-2019\n",
    "balance_table_new_18to19 = balance_table_new[['29/06/2019']].copy()\n",
    "#2019-2020\n",
    "balance_table_new_19to20 = balance_table_new[['29/06/2020']].copy()\n",
    "#2020-2021\n",
    "balance_table_new_20to21 = balance_table_new[['29/06/2021']].copy()\n",
    "\n",
    "# Convert them into dictionaries\n",
    "balance_table_new_17to18_dict = balance_table_new_17to18.to_dict()['29/06/2018']\n",
    "balance_table_new_18to19_dict = balance_table_new_18to19.to_dict()['29/06/2019']\n",
    "balance_table_new_19to20_dict = balance_table_new_19to20.to_dict()['29/06/2020']\n",
    "balance_table_new_20to21_dict = balance_table_new_20to21.to_dict()['29/06/2021']\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "# Insert into MongoDB\n",
    "stock_balance_reports = {'29/06/2018':balance_table_new_17to18_dict,\n",
    "                         '29/06/2019':balance_table_new_18to19_dict,\n",
    "                         '29/06/2020':balance_table_new_19to20_dict,\n",
    "                         '29/06/2021':balance_table_new_20to21_dict}\n",
    "\n",
    "stock_dict[stock] = stock_balance_reports\n",
    "\n",
    "client.Stocks_db.balance_sheets.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  CBA  ###############\n",
    "\n",
    "############################################################\n",
    "\n",
    "###############  ANZ  ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'ANZ'\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/balance-sheet?p={stock}.AX&.tsrc=fin-srch'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "balance_sheet_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "balance_sheet_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in balance_sheet_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "\n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in balance_sheet_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Convert the result into a DataFrame\n",
    "balance_table = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "balance_table_new = balance_table[balance_table['29/09/2020']!=''].copy()\n",
    "# Replace all fields containing '-' with 0\n",
    "balance_table_new = balance_table_new.replace('-',0)\n",
    "# Set the index to Breakdown\n",
    "balance_table_new = balance_table_new.set_index('Breakdown')\n",
    "\n",
    "# Divide into series by financial year\n",
    "#2017-2018\n",
    "balance_table_new_17to18 = balance_table_new[['29/09/2018']].copy()\n",
    "#2018-2019\n",
    "balance_table_new_18to19 = balance_table_new[['29/09/2019']].copy()\n",
    "#2019-2020\n",
    "balance_table_new_19to20 = balance_table_new[['29/09/2020']].copy()\n",
    "\n",
    "# Convert to dictionaries\n",
    "balance_table_new_17to18_dict = balance_table_new_17to18.to_dict()['29/09/2018']\n",
    "balance_table_new_18to19_dict = balance_table_new_18to19.to_dict()['29/09/2019']\n",
    "balance_table_new_19to20_dict = balance_table_new_19to20.to_dict()['29/09/2020']\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "# Insert into MongoDB\n",
    "stock_balance_reports = {'29/06/2018':balance_table_new_17to18_dict,\n",
    "                         '29/06/2019':balance_table_new_18to19_dict,\n",
    "                         '29/06/2020':balance_table_new_19to20_dict}\n",
    "\n",
    "stock_dict[stock] = stock_balance_reports\n",
    "\n",
    "client.Stocks_db.balance_sheets.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  ANZ  ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2682832",
   "metadata": {},
   "source": [
    "## Stocks Cash Flow - ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  CBA  ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'CBA'\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/cash-flow?p={stock}.AX'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "cashflow_table_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "cashflow_table_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in cashflow_table_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "\n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in cashflow_table_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Convert the result into a DataFrame\n",
    "cashflow_table = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "cashflow_table_new = cashflow_table[cashflow_table['29/06/2021']!=''].copy()\n",
    "# Replace all fields containing '-' with 0\n",
    "cashflow_table_new = cashflow_table_new.replace('-',0)\n",
    "# Set the index to Breakdown\n",
    "cashflow_table_new = cashflow_table_new.set_index('Breakdown')\n",
    "\n",
    "# Divide into series by financial year\n",
    "#2017-2018\n",
    "cashflow_table_new_17to18 = cashflow_table_new[['29/06/2018']].copy()\n",
    "#2018-2019\n",
    "cashflow_table_new_18to19 = cashflow_table_new[['29/06/2019']].copy()\n",
    "#2019-2020\n",
    "cashflow_table_new_19to20 = cashflow_table_new[['29/06/2020']].copy()\n",
    "#2020-2021\n",
    "cashflow_table_new_20to21 = cashflow_table_new[['29/06/2021']].copy()\n",
    "# TTM\n",
    "cashflow_table_new_ttm = cashflow_table_new[['ttm']].copy()\n",
    "\n",
    "# Convert them to dictionaries\n",
    "cashflow_table_new_17to18_dict = cashflow_table_new_17to18.to_dict()['29/06/2018']\n",
    "cashflow_table_new_18to19_dict = cashflow_table_new_18to19.to_dict()['29/06/2019']\n",
    "cashflow_table_new_19to20_dict = cashflow_table_new_19to20.to_dict()['29/06/2020']\n",
    "cashflow_table_new_20to21_dict = cashflow_table_new_20to21.to_dict()['29/06/2021']\n",
    "cashflow_table_new_ttm_dict = cashflow_table_new_ttm.to_dict()['ttm']\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "# Load Into Mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Load all dictionaries into a single dictionary\n",
    "stock_cash_flow_reports = {'29/06/2018':cashflow_table_new_17to18_dict,\n",
    "                         '29/06/2019':cashflow_table_new_18to19_dict,\n",
    "                         '29/06/2020':cashflow_table_new_19to20_dict,\n",
    "                         '29/06/2021':cashflow_table_new_20to21_dict,\n",
    "                         'ttm':cashflow_table_new_ttm_dict}\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "stock_dict[stock] = stock_cash_flow_reports\n",
    "\n",
    "# Insert into Mongo db\n",
    "client.Stocks_db.cash_flow.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  CBA   ###############\n",
    "\n",
    "############################################################\n",
    "\n",
    "###############  ANZ   ###############\n",
    "##### Extract #####\n",
    "# Read the tables in the HTML page\n",
    "# Scrape the Data\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "stock = 'ANZ'\n",
    "url = f'https://au.finance.yahoo.com/quote/{stock}.AX/cash-flow?p={stock}.AX'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get table headers\n",
    "cashflow_table_headers = soup.find_all('div', class_='D(tbr) C($primaryColor)')\n",
    "# Get table rows\n",
    "cashflow_table_rows = soup.find_all('div', class_='D(tbr) fi-row Bgc($hoverBgColor):h')\n",
    "\n",
    "# Quit the browser\n",
    "browser.quit()\n",
    "\n",
    "# Get the table headers\n",
    "table_headers_list = []\n",
    "for header in cashflow_table_headers:\n",
    "    for span in header.find_all('span'):\n",
    "        table_headers_list.append(span.text)\n",
    "\n",
    "# Get all the table rows\n",
    "table_rows = []\n",
    "row_list = []\n",
    "for rows in cashflow_table_rows:\n",
    "    for fields in rows:\n",
    "        row_list.append(fields.text)\n",
    "    # Add row to table rows\n",
    "    table_rows.append(row_list)\n",
    "    # Reset the row list for the next row\n",
    "    row_list = []\n",
    "    \n",
    "# Convert the result into a DataFrame\n",
    "cashflow_table = pd.DataFrame(table_rows, columns=table_headers_list)\n",
    "##### Extract #####\n",
    "##### Transform #####\n",
    "# Remove all the rows without values based on any of the financial year columns\n",
    "cashflow_table_new = cashflow_table[cashflow_table['29/09/2020']!=''].copy()\n",
    "# Replace all fields containing '-' with 0\n",
    "cashflow_table_new = cashflow_table_new.replace('-',0)\n",
    "# Set the index to Breakdown\n",
    "cashflow_table_new = cashflow_table_new.set_index('Breakdown')\n",
    "\n",
    "# Divide into series by financial year\n",
    "#2017-2018\n",
    "cashflow_table_new_17to18 = cashflow_table_new[['29/09/2018']].copy()\n",
    "#2018-2019\n",
    "cashflow_table_new_18to19 = cashflow_table_new[['29/09/2019']].copy()\n",
    "#2019-2020\n",
    "cashflow_table_new_19to20 = cashflow_table_new[['29/09/2020']].copy()\n",
    "# TTM\n",
    "cashflow_table_new_ttm = cashflow_table_new[['ttm']].copy()\n",
    "\n",
    "cashflow_table_new_17to18_dict = cashflow_table_new_17to18.to_dict()['29/09/2018']\n",
    "cashflow_table_new_18to19_dict = cashflow_table_new_18to19.to_dict()['29/09/2019']\n",
    "cashflow_table_new_19to20_dict = cashflow_table_new_19to20.to_dict()['29/09/2020']\n",
    "cashflow_table_new_ttm_dict = cashflow_table_new_ttm.to_dict()['ttm']\n",
    "\n",
    "##### Transform #####\n",
    "\n",
    "##### Load #####\n",
    "# Load Into Mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "stock_dict = {}\n",
    "\n",
    "# Load all dictionaries into a single dictionary\n",
    "stock_cash_flow_reports = {'29/09/2018':cashflow_table_new_17to18_dict,\n",
    "                         '29/09/2019':cashflow_table_new_18to19_dict,\n",
    "                         '29/09/2020':cashflow_table_new_19to20_dict,\n",
    "                         'ttm':cashflow_table_new_ttm_dict}\n",
    "\n",
    "stock_dict[stock] = stock_cash_flow_reports\n",
    "\n",
    "# Insert into MongoDB\n",
    "client.Stocks_db.cash_flow.insert_one(stock_dict)\n",
    "##### Load #####\n",
    "###############  ANZ   ###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
